{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
    "\n",
    "# Paths\n",
    "dataset_path = \"your_path\"\n",
    "results_path = \"your_path\"\n",
    "\n",
    "# Load the dataset\n",
    "def load_data(dataset_path):\n",
    "    motion_data, labels = [], []\n",
    "    for file_name in sorted(os.listdir(dataset_path)):\n",
    "        if file_name.startswith(\"._\"):  # Skip macOS metadata files\n",
    "            continue\n",
    "        file_path = os.path.join(dataset_path, file_name)\n",
    "        if file_name.startswith(\"moc_s\") and \"t-pose\" not in file_name:\n",
    "            try:\n",
    "                parts = file_name.split(\"_\")\n",
    "                action_id = int(parts[2][1:])  # Extract action ID (e.g., \"a01\" -> 1)\n",
    "                data = np.loadtxt(file_path, encoding=\"latin1\")\n",
    "                motion_data.append(data)\n",
    "                labels.append(action_id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading motion file {file_name}: {e}\")\n",
    "    return np.array(motion_data, dtype=object), np.array(labels)\n",
    "\n",
    "# Preprocess the data with pairwise distances and angular velocities\n",
    "def preprocess_data(motion_data, labels, sequence_length=100):\n",
    "    processed_data, processed_labels = [], []\n",
    "    for sequence, label in zip(motion_data, labels):\n",
    "        if sequence.shape[0] > sequence_length:\n",
    "            sequence = sequence[:sequence_length, :]\n",
    "        elif sequence.shape[0] < sequence_length:\n",
    "            padding = np.zeros((sequence_length - sequence.shape[0], sequence.shape[1]))\n",
    "            sequence = np.vstack((sequence, padding))\n",
    "        \n",
    "        # Pairwise joint distances\n",
    "        num_joints = sequence.shape[1] // 3  # Assuming (x, y, z) for each joint\n",
    "        distances = []\n",
    "        for i in range(num_joints):\n",
    "            for j in range(i + 1, num_joints):\n",
    "                dist = np.linalg.norm(sequence[:, 3 * i:3 * (i + 1)] - sequence[:, 3 * j:3 * (j + 1)], axis=1)\n",
    "                distances.append(dist)\n",
    "        distances = np.array(distances).T\n",
    "\n",
    "        # Angular velocities\n",
    "        velocities = np.diff(sequence, axis=0, prepend=sequence[0:1])\n",
    "        angular_velocity = np.linalg.norm(np.cross(sequence[:, :3], velocities[:, :3]), axis=1, keepdims=True)\n",
    "        \n",
    "        # Concatenate features\n",
    "        sequence = np.hstack((sequence, distances, angular_velocity))\n",
    "        sequence = (sequence - np.mean(sequence, axis=0)) / (np.std(sequence, axis=0) + 1e-6)  # Normalize\n",
    "        \n",
    "        processed_data.append(sequence)\n",
    "        processed_labels.append(label - 1)  # Convert 1-based labels to 0-based\n",
    "    return np.array(processed_data), np.array(processed_labels)\n",
    "\n",
    "# Data Augmentation\n",
    "def augment_data(data):\n",
    "    augmented_data = []\n",
    "    for sequence in data:\n",
    "        # Apply random scaling\n",
    "        scaled_sequence = sequence * (1 + np.random.uniform(-0.1, 0.1, sequence.shape))\n",
    "        # Add Gaussian noise\n",
    "        noisy_sequence = scaled_sequence + np.random.normal(0, 0.01, size=sequence.shape)\n",
    "        # Temporal shifting\n",
    "        shift = np.roll(noisy_sequence, np.random.randint(1, 5), axis=0)\n",
    "        augmented_data.append(shift)\n",
    "    return np.array(augmented_data)\n",
    "\n",
    "# Advanced ST-GCN Model\n",
    "def build_advanced_stgcn(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Spatial-Temporal Graph Convolutional Layers\n",
    "    x = layers.Conv1D(64, kernel_size=3, activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.Conv1D(128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.Conv1D(256, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n",
    "    \n",
    "    # Attention Mechanism\n",
    "    attention = layers.MultiHeadAttention(num_heads=4, key_dim=256)(x, x)\n",
    "    x = layers.Add()([x, attention])\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Dense Layers\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "motion_data, labels = load_data(dataset_path)\n",
    "X, y = preprocess_data(motion_data, labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Augment training data\n",
    "X_train_augmented = augment_data(X_train)\n",
    "X_train_combined = np.concatenate([X_train, X_train_augmented])\n",
    "y_train_combined = np.concatenate([y_train, y_train])\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Build and compile the model\n",
    "input_shape = X_train_combined.shape[1:]  # (sequence_length, num_features)\n",
    "num_classes = len(np.unique(y))\n",
    "model = build_advanced_stgcn(input_shape, num_classes)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_combined, y_train_combined,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Save results and figures\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "results = {\"Test Accuracy\": test_accuracy, \"Test Loss\": test_loss}\n",
    "with open(os.path.join(results_path, \"results.txt\"), \"w\") as f:\n",
    "    for key, value in results.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "# Save the model\n",
    "model.save(os.path.join(results_path, \"action_recognition_advanced_stgcn_model.h5\"))\n",
    "\n",
    "# Plot training history\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(results_path, \"training_history.png\"))\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(results_path, \"loss_history.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Additional evaluation metrics\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "report = classification_report(y_test, y_pred, target_names=[f\"Class {i}\" for i in range(num_classes)])\n",
    "print(report)\n",
    "\n",
    "# Save classification report\n",
    "with open(os.path.join(results_path, \"classification_report.txt\"), \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[f\"Class {i}\" for i in range(num_classes)])\n",
    "plt.figure()\n",
    "disp.plot(cmap=\"viridis\", xticks_rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_path, \"confusion_matrix.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1-Score bar chart\n",
    "precisions, recalls, f1_scores, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "plt.figure()\n",
    "x = np.arange(num_classes)\n",
    "plt.bar(x - 0.2, precisions, width=0.2, label=\"Precision\")\n",
    "plt.bar(x, recalls, width=0.2, label=\"Recall\")\n",
    "plt.bar(x + 0.2, f1_scores, width=0.2, label=\"F1-Score\")\n",
    "plt.xticks(x, [f\"Class {i}\" for i in range(num_classes)], rotation=45)\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_path, \"precision_recall_f1.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Predicted vs True label distribution\n",
    "plt.figure()\n",
    "plt.hist([y_test, y_pred], bins=num_classes, label=[\"True Labels\", \"Predicted Labels\"], color=[\"blue\", \"orange\"], alpha=0.7)\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_path, \"label_distribution.png\"))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
